{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\finetuner\\My_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision\n",
    "from transformer.Vision_transformer import VisionTransformer\n",
    "import torchvision.transforms as transforms\n",
    "from pprint import pprint\n",
    "from torchsummary import summary\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \"\"\"Puts incoming MNIST dataset into an object \n",
    "        which can be loaded onto cuda gpu.\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : torchvision.datasets.mnist.MNIST\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    X : torch.Tensor\n",
    "        Shape `(n_samples, n_channels, img_height, img_width)`\n",
    "    \"\"\"\n",
    "    def __init__(self, data, device = device):\n",
    "        self.X = torch.cat([torch.unsqueeze(data[i][0], dim=0) for i in range(len(data))], dim=0).to(device)\n",
    "        self.Y = torch.tensor([data[i][1] for i in range(len(data))]).to(device)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Length method.\n",
    "        Parameters\n",
    "        ----------\n",
    "        None\n",
    "        Returns\n",
    "        ----------\n",
    "        int\n",
    "            n_samples\n",
    "\n",
    "        \"\"\"\n",
    "        return self.X.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Indexing call.\n",
    "        Parameters:\n",
    "        idx : int\n",
    "            index of element to be returned.\n",
    "        \n",
    "        Returns : \n",
    "        torch.Tensor\n",
    "            Shape `(n_channels, img_height, img_width)`\n",
    "        torch.Tensor\n",
    "            Shape `(class_idx)`\n",
    "        \"\"\"\n",
    "        return self.X[idx], self.Y[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])   # Transform object to apply on the dataset.\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "# Loading/Downloading dataset. `download` can be `False` if the data is present in the root directory\n",
    "# Else it will download the dataset to to the root location.\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Getting the device to compute on. `cuda` if GPU is available, else `cpu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert PIL Image to tensor\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize the image tensors\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 training dataset\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "# Load CIFAR-10 test dataset\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Classes in CIFAR-10 dataset\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = CustomDataset(data=train_dataset)\n",
    "test_ds = CustomDataset(data=test_dataset)\n",
    "# Made custom dataset objects from the MNIST dataset.\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_ds, batch_size=64, shuffle=False)\n",
    "# DataLoaders for fast implementation of loading batch-wise data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "CIFAR_ViT = VisionTransformer(\n",
    "    img_size=32,\n",
    "    patch_size=8,\n",
    "    in_chans=3,\n",
    "    n_classes=len(classes),\n",
    "    embed_dim=128,\n",
    "    depth=4,\n",
    "    n_heads=4,\n",
    "    mlp_ratio=0.5,\n",
    "    p=0.3,\n",
    "    attn_p=0.3\n",
    ").to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 128, 4, 4]          24,704\n",
      "        PatchEmbed-2              [-1, 16, 128]               0\n",
      "           Dropout-3              [-1, 17, 128]               0\n",
      "         LayerNorm-4              [-1, 17, 128]             256\n",
      "            Linear-5              [-1, 17, 384]          49,536\n",
      "           Dropout-6            [-1, 4, 17, 17]               0\n",
      "            Linear-7              [-1, 17, 128]          16,512\n",
      "           Dropout-8              [-1, 17, 128]               0\n",
      "         Attention-9              [-1, 17, 128]               0\n",
      "        LayerNorm-10              [-1, 17, 128]             256\n",
      "           Linear-11               [-1, 17, 64]           8,256\n",
      "             GELU-12               [-1, 17, 64]               0\n",
      "          Dropout-13               [-1, 17, 64]               0\n",
      "           Linear-14              [-1, 17, 128]           8,320\n",
      "          Dropout-15              [-1, 17, 128]               0\n",
      "              MLP-16              [-1, 17, 128]               0\n",
      "            Block-17              [-1, 17, 128]               0\n",
      "        LayerNorm-18              [-1, 17, 128]             256\n",
      "           Linear-19              [-1, 17, 384]          49,536\n",
      "          Dropout-20            [-1, 4, 17, 17]               0\n",
      "           Linear-21              [-1, 17, 128]          16,512\n",
      "          Dropout-22              [-1, 17, 128]               0\n",
      "        Attention-23              [-1, 17, 128]               0\n",
      "        LayerNorm-24              [-1, 17, 128]             256\n",
      "           Linear-25               [-1, 17, 64]           8,256\n",
      "             GELU-26               [-1, 17, 64]               0\n",
      "          Dropout-27               [-1, 17, 64]               0\n",
      "           Linear-28              [-1, 17, 128]           8,320\n",
      "          Dropout-29              [-1, 17, 128]               0\n",
      "              MLP-30              [-1, 17, 128]               0\n",
      "            Block-31              [-1, 17, 128]               0\n",
      "        LayerNorm-32              [-1, 17, 128]             256\n",
      "           Linear-33              [-1, 17, 384]          49,536\n",
      "          Dropout-34            [-1, 4, 17, 17]               0\n",
      "           Linear-35              [-1, 17, 128]          16,512\n",
      "          Dropout-36              [-1, 17, 128]               0\n",
      "        Attention-37              [-1, 17, 128]               0\n",
      "        LayerNorm-38              [-1, 17, 128]             256\n",
      "           Linear-39               [-1, 17, 64]           8,256\n",
      "             GELU-40               [-1, 17, 64]               0\n",
      "          Dropout-41               [-1, 17, 64]               0\n",
      "           Linear-42              [-1, 17, 128]           8,320\n",
      "          Dropout-43              [-1, 17, 128]               0\n",
      "              MLP-44              [-1, 17, 128]               0\n",
      "            Block-45              [-1, 17, 128]               0\n",
      "        LayerNorm-46              [-1, 17, 128]             256\n",
      "           Linear-47              [-1, 17, 384]          49,536\n",
      "          Dropout-48            [-1, 4, 17, 17]               0\n",
      "           Linear-49              [-1, 17, 128]          16,512\n",
      "          Dropout-50              [-1, 17, 128]               0\n",
      "        Attention-51              [-1, 17, 128]               0\n",
      "        LayerNorm-52              [-1, 17, 128]             256\n",
      "           Linear-53               [-1, 17, 64]           8,256\n",
      "             GELU-54               [-1, 17, 64]               0\n",
      "          Dropout-55               [-1, 17, 64]               0\n",
      "           Linear-56              [-1, 17, 128]           8,320\n",
      "          Dropout-57              [-1, 17, 128]               0\n",
      "              MLP-58              [-1, 17, 128]               0\n",
      "            Block-59              [-1, 17, 128]               0\n",
      "        LayerNorm-60              [-1, 17, 128]             256\n",
      "           Linear-61                   [-1, 10]           1,290\n",
      "================================================================\n",
      "Total params: 358,794\n",
      "Trainable params: 358,794\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.00\n",
      "Params size (MB): 1.37\n",
      "Estimated Total Size (MB): 2.38\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(CIFAR_ViT, (3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Loss criteria for multiclass classification task.\n",
    "optimizer = torch.optim.Adam(CIFAR_ViT.parameters(), lr=0.001)\n",
    "# Optimizer to update weights after calculating gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/150], Loss: 1.8505\n",
      "Epoch [2/150], Loss: 1.6677\n",
      "Epoch [3/150], Loss: 1.5761\n",
      "Epoch [4/150], Loss: 1.5164\n",
      "Epoch [5/150], Loss: 1.4654\n",
      "Epoch [6/150], Loss: 1.4317\n",
      "Epoch [7/150], Loss: 1.3975\n",
      "Epoch [8/150], Loss: 1.3718\n",
      "Epoch [9/150], Loss: 1.3386\n",
      "Epoch [10/150], Loss: 1.3196\n",
      "Epoch [11/150], Loss: 1.2947\n",
      "Epoch [12/150], Loss: 1.2717\n",
      "Epoch [13/150], Loss: 1.2457\n",
      "Epoch [14/150], Loss: 1.2294\n",
      "Epoch [15/150], Loss: 1.2054\n",
      "Epoch [16/150], Loss: 1.1933\n",
      "Epoch [17/150], Loss: 1.1777\n",
      "Epoch [18/150], Loss: 1.1518\n",
      "Epoch [19/150], Loss: 1.1354\n",
      "Epoch [20/150], Loss: 1.1202\n",
      "Epoch [21/150], Loss: 1.1075\n",
      "Epoch [22/150], Loss: 1.0885\n",
      "Epoch [23/150], Loss: 1.0814\n",
      "Epoch [24/150], Loss: 1.0680\n",
      "Epoch [25/150], Loss: 1.0547\n",
      "Epoch [26/150], Loss: 1.0392\n",
      "Epoch [27/150], Loss: 1.0359\n",
      "Epoch [28/150], Loss: 1.0177\n",
      "Epoch [29/150], Loss: 1.0112\n",
      "Epoch [30/150], Loss: 1.0030\n",
      "Epoch [31/150], Loss: 0.9932\n",
      "Epoch [32/150], Loss: 0.9853\n",
      "Epoch [33/150], Loss: 0.9831\n",
      "Epoch [34/150], Loss: 0.9732\n",
      "Epoch [35/150], Loss: 0.9648\n",
      "Epoch [36/150], Loss: 0.9616\n",
      "Epoch [37/150], Loss: 0.9496\n",
      "Epoch [38/150], Loss: 0.9419\n",
      "Epoch [39/150], Loss: 0.9333\n",
      "Epoch [40/150], Loss: 0.9330\n",
      "Epoch [41/150], Loss: 0.9240\n",
      "Epoch [42/150], Loss: 0.9154\n",
      "Epoch [43/150], Loss: 0.9074\n",
      "Epoch [44/150], Loss: 0.9052\n",
      "Epoch [45/150], Loss: 0.9020\n",
      "Epoch [46/150], Loss: 0.8915\n",
      "Epoch [47/150], Loss: 0.8890\n",
      "Epoch [48/150], Loss: 0.8806\n",
      "Epoch [49/150], Loss: 0.8754\n",
      "Epoch [50/150], Loss: 0.8724\n",
      "Epoch [51/150], Loss: 0.8675\n",
      "Epoch [52/150], Loss: 0.8643\n",
      "Epoch [53/150], Loss: 0.8588\n",
      "Epoch [54/150], Loss: 0.8516\n",
      "Epoch [55/150], Loss: 0.8477\n",
      "Epoch [56/150], Loss: 0.8450\n",
      "Epoch [57/150], Loss: 0.8412\n",
      "Epoch [58/150], Loss: 0.8403\n",
      "Epoch [59/150], Loss: 0.8296\n",
      "Epoch [60/150], Loss: 0.8338\n",
      "Epoch [61/150], Loss: 0.8253\n",
      "Epoch [62/150], Loss: 0.8198\n",
      "Epoch [63/150], Loss: 0.8163\n",
      "Epoch [64/150], Loss: 0.8118\n",
      "Epoch [65/150], Loss: 0.8082\n",
      "Epoch [66/150], Loss: 0.8047\n",
      "Epoch [67/150], Loss: 0.8038\n",
      "Epoch [68/150], Loss: 0.8005\n",
      "Epoch [69/150], Loss: 0.7969\n",
      "Epoch [70/150], Loss: 0.7930\n",
      "Epoch [71/150], Loss: 0.7874\n",
      "Epoch [72/150], Loss: 0.7893\n",
      "Epoch [73/150], Loss: 0.7816\n",
      "Epoch [74/150], Loss: 0.7787\n",
      "Epoch [75/150], Loss: 0.7781\n",
      "Epoch [76/150], Loss: 0.7736\n",
      "Epoch [77/150], Loss: 0.7720\n",
      "Epoch [78/150], Loss: 0.7695\n",
      "Epoch [79/150], Loss: 0.7624\n",
      "Epoch [80/150], Loss: 0.7641\n",
      "Epoch [81/150], Loss: 0.7610\n",
      "Epoch [82/150], Loss: 0.7610\n",
      "Epoch [83/150], Loss: 0.7525\n",
      "Epoch [84/150], Loss: 0.7555\n",
      "Epoch [85/150], Loss: 0.7492\n",
      "Epoch [86/150], Loss: 0.7459\n",
      "Epoch [87/150], Loss: 0.7458\n",
      "Epoch [88/150], Loss: 0.7434\n",
      "Epoch [89/150], Loss: 0.7395\n",
      "Epoch [90/150], Loss: 0.7346\n",
      "Epoch [91/150], Loss: 0.7346\n",
      "Epoch [92/150], Loss: 0.7307\n",
      "Epoch [93/150], Loss: 0.7331\n",
      "Epoch [94/150], Loss: 0.7275\n",
      "Epoch [95/150], Loss: 0.7278\n",
      "Epoch [96/150], Loss: 0.7260\n",
      "Epoch [97/150], Loss: 0.7235\n",
      "Epoch [98/150], Loss: 0.7184\n",
      "Epoch [99/150], Loss: 0.7210\n",
      "Epoch [100/150], Loss: 0.7148\n",
      "Epoch [101/150], Loss: 0.7169\n",
      "Epoch [102/150], Loss: 0.7136\n",
      "Epoch [103/150], Loss: 0.7100\n",
      "Epoch [104/150], Loss: 0.7076\n",
      "Epoch [105/150], Loss: 0.7058\n",
      "Epoch [106/150], Loss: 0.7025\n",
      "Epoch [107/150], Loss: 0.7002\n",
      "Epoch [108/150], Loss: 0.6991\n",
      "Epoch [109/150], Loss: 0.6996\n",
      "Epoch [110/150], Loss: 0.6977\n",
      "Epoch [111/150], Loss: 0.6941\n",
      "Epoch [112/150], Loss: 0.6913\n",
      "Epoch [113/150], Loss: 0.6861\n",
      "Epoch [114/150], Loss: 0.6860\n",
      "Epoch [115/150], Loss: 0.6791\n",
      "Epoch [116/150], Loss: 0.6878\n",
      "Epoch [117/150], Loss: 0.6827\n",
      "Epoch [118/150], Loss: 0.6762\n",
      "Epoch [119/150], Loss: 0.6749\n",
      "Epoch [120/150], Loss: 0.6775\n",
      "Epoch [121/150], Loss: 0.6770\n",
      "Epoch [122/150], Loss: 0.6779\n",
      "Epoch [123/150], Loss: 0.6733\n",
      "Epoch [124/150], Loss: 0.6680\n",
      "Epoch [125/150], Loss: 0.6672\n",
      "Epoch [126/150], Loss: 0.6690\n",
      "Epoch [127/150], Loss: 0.6638\n",
      "Epoch [128/150], Loss: 0.6626\n",
      "Epoch [129/150], Loss: 0.6598\n",
      "Epoch [130/150], Loss: 0.6602\n",
      "Epoch [131/150], Loss: 0.6561\n",
      "Epoch [132/150], Loss: 0.6588\n",
      "Epoch [133/150], Loss: 0.6581\n",
      "Epoch [134/150], Loss: 0.6486\n",
      "Epoch [135/150], Loss: 0.6530\n",
      "Epoch [136/150], Loss: 0.6469\n",
      "Epoch [137/150], Loss: 0.6443\n",
      "Epoch [138/150], Loss: 0.6476\n",
      "Epoch [139/150], Loss: 0.6472\n",
      "Epoch [140/150], Loss: 0.6464\n",
      "Epoch [141/150], Loss: 0.6402\n",
      "Epoch [142/150], Loss: 0.6406\n",
      "Epoch [143/150], Loss: 0.6391\n",
      "Epoch [144/150], Loss: 0.6373\n",
      "Epoch [145/150], Loss: 0.6385\n",
      "Epoch [146/150], Loss: 0.6390\n",
      "Epoch [147/150], Loss: 0.6318\n",
      "Epoch [148/150], Loss: 0.6351\n",
      "Epoch [149/150], Loss: 0.6343\n",
      "Epoch [150/150], Loss: 0.6276\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 150\n",
    "# Number of Epochs to run the following training loop for\n",
    "for epoch in range(num_epochs):\n",
    "    CIFAR_ViT.train()\n",
    "    # Setting the model in training mode\n",
    "    running_loss = 0.0  \n",
    "    # Parameter to store the total loss over dataset in the epoch. This has no role in training.\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        #loading images and labels to device. In our case, it is the cuda GPU device.\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = CIFAR_ViT(images)\n",
    "        # Predicting classes of the input batch.\n",
    "        loss = criterion(outputs, labels)\n",
    "        # Calculating loss of the predicted classes with the ground truth\n",
    "        loss.backward()\n",
    "        # Backpropagation step\n",
    "        optimizer.step()\n",
    "        # Updating the weights according to the optimizer's rules.\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        # Calculating the loss over the dataset\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, loader = test_loader):\n",
    "    correct, total = 0, 0\n",
    "    model.eval()\n",
    "    # Setting the model in evaluation mode.\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            # Loading batch images and ground truth onto device\n",
    "            outputs = model(images)\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "    return f\"Accuracy on test set: {(100 * correct / total):.2f}%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Accuracy on test set: 68.14%'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(CIFAR_ViT, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'model_state_dict': CIFAR_ViT.state_dict()\n",
    "}, \"cifar_model_86pct.pth\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
