{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\finetuner\\My_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision\n",
    "from transformer.Vision_transformer import VisionTransformer, CustomDataset\n",
    "import torchvision.transforms as transforms\n",
    "from pprint import pprint\n",
    "from torchsummary import summary\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "from torch.ao.quantization import get_default_qconfig\n",
    "from torch.ao.quantization.quantize_fx import prepare_fx, convert_fx\n",
    "from torch.ao.quantization import QConfigMapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device : cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cpu\")\n",
    "print(f\"Using device : {device}\")\n",
    "# We don't want to perform our quantization step on cuda GPU. It is not supported.\n",
    "with open('transformer/config.json') as f:\n",
    "    custom_config = json.load(f)\n",
    "# Custom configurations for the VisionTransformer.\n",
    "# Transformer can be customized with these configurations.\n",
    "# Refer to documentation of the class VisionTransformer\n",
    "# (`VisionTransformer.__doc__`, use pprint for cleaner display)\n",
    "# for exact details of the customization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load saved model\n",
    "float_model = VisionTransformer(**custom_config).to(device=device)\n",
    "\n",
    "# float_model = VisionTransformer(\n",
    "#     img_size=32,\n",
    "#     patch_size=8,\n",
    "#     in_chans=3,\n",
    "#     n_classes=10,\n",
    "#     embed_dim=128,\n",
    "#     depth=2,\n",
    "#     n_heads=2,\n",
    "#     mlp_ratio=4.,\n",
    "#     p=0.3,\n",
    "#     attn_p=0.3\n",
    "# ).to(device=device)\n",
    "checkpoint = torch.load(\"models/model.pth\")\n",
    "float_model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])   # Transform object to apply on the dataset.\n",
    "\n",
    "# train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Loading/Downloading dataset. `download` can be `False` if the data is present in the root directory\n",
    "# Else it will download the dataset to to the root location.\n",
    "\n",
    "train_ds = CustomDataset(data=train_dataset)\n",
    "test_ds = CustomDataset(data=test_dataset, device=device)\n",
    "# Made custom dataset objects from the MNIST dataset.\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_ds, batch_size=64, shuffle=False)\n",
    "# DataLoaders for fast implementation of loading batch-wise data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform = transforms.Compose([\n",
    "#     transforms.ToTensor(),  # Convert PIL Image to tensor\n",
    "#     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize the image tensors\n",
    "# ])\n",
    "\n",
    "# # Load CIFAR-10 training dataset\n",
    "# train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "# # Load CIFAR-10 test dataset\n",
    "# test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# # Classes in CIFAR-10 dataset\n",
    "# classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "\n",
    "# train_ds = CustomDataset(data=train_dataset)\n",
    "# test_ds = CustomDataset(data=test_dataset)\n",
    "# # Made custom dataset objects from the MNIST dataset.\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(train_ds, batch_size=2048, shuffle=True)\n",
    "# test_loader = torch.utils.data.DataLoader(test_ds, batch_size=2048, shuffle=False)\n",
    "# # DataLoaders for fast implementation of loading batch-wise data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # qconfig = get_default_qconfig(\"x86\")\n",
    "\n",
    "# max_bit_length = 4\n",
    "# qconfig = torch.quantization.QConfig(\n",
    "#     activation=torch.quantization.fake_quantize.FakeQuantize.with_args(observer = torch.quantization.observer.MovingAverageMinMaxObserver.with_args(dtype=torch.quint8), quant_min = 0 ,quant_max=2**(max_bit_length)-1, dtype=torch.quint8), \n",
    "#     weight=torch.quantization.fake_quantize.FakeQuantize.with_args(observer = torch.quantization.observer.MovingAverageMinMaxObserver.with_args(dtype=torch.qint8), quant_min = 0 ,quant_max=2**(max_bit_length)-1, dtype=torch.qint8)\n",
    "# )\n",
    "# qconfig_mapping = QConfigMapping().set_global(qconfig)\n",
    "\n",
    "# def calibrate(model, data_loader):\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         for image, _ in data_loader:\n",
    "#             image = image.to(device)\n",
    "#             model(image)\n",
    "\n",
    "\n",
    "# example_inputs = (next(iter(test_loader))[0]) # get an example input\n",
    "# prepared_model = prepare_fx(float_model, qconfig_mapping, example_inputs=example_inputs ) \n",
    "\n",
    "# calibrate(prepared_model, train_loader)\n",
    "# quantized_model = convert_fx(prepared_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qconfig = get_default_qconfig(\"x86\")\n",
    "\n",
    "max_bit_length = 4\n",
    "qconfig = torch.quantization.QConfig(\n",
    "    activation=torch.quantization.fake_quantize.FakeQuantize.with_args(observer = torch.quantization.observer.MovingAverageMinMaxObserver.with_args(dtype=torch.quint8), quant_min = 0 ,quant_max=2**(max_bit_length)-1, dtype=torch.quint8), \n",
    "    weight=torch.quantization.fake_quantize.FakeQuantize.with_args(observer = torch.quantization.observer.MovingAverageMinMaxObserver.with_args(dtype=torch.qint8), quant_min = 0 ,quant_max=2**(max_bit_length)-1, dtype=torch.qint8)\n",
    ")\n",
    "qconfig_mapping = QConfigMapping().set_global(qconfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QConfigMapping (\n",
       " global_qconfig\n",
       "  QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FakeQuantize'>, observer=functools.partial(<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, dtype=torch.quint8){}, quant_min=0, quant_max=15, dtype=torch.quint8){}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FakeQuantize'>, observer=functools.partial(<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, dtype=torch.qint8){}, quant_min=0, quant_max=15, dtype=torch.qint8){})\n",
       " object_type_qconfigs\n",
       "  OrderedDict()\n",
       " module_name_regex_qconfigs\n",
       "  OrderedDict()\n",
       " module_name_qconfigs\n",
       "  OrderedDict()\n",
       " module_name_object_type_order_qconfigs\n",
       "  OrderedDict()\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qconfig_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calibrate(model, data_loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for image, _ in data_loader:\n",
    "            image = image.to(device)\n",
    "            model(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_inputs = (next(iter(test_loader))[0]) # get an example input\n",
    "prepared_model = prepare_fx(float_model, qconfig_mapping, example_inputs=example_inputs ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibrate(prepared_model, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model = convert_fx(prepared_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cls_token Parameter containing:\n",
      "tensor([[[-0.0036,  0.0393,  0.0284,  0.0412,  0.0239, -0.0122,  0.0125,\n",
      "           0.0209,  0.0172, -0.0591, -0.0249, -0.0518,  0.0008, -0.0724,\n",
      "           0.0681,  0.0243, -0.2622,  0.6902,  0.0107, -0.0901, -0.1576,\n",
      "          -0.0022,  0.0790, -0.0144,  0.0019, -0.0107,  0.0219, -0.0109,\n",
      "           0.0872, -0.0216,  0.0366,  0.0085]]], requires_grad=True)\n",
      "pos_embed Parameter containing:\n",
      "tensor([[[-0.0036,  0.0393,  0.0284,  ..., -0.0216,  0.0366,  0.0085],\n",
      "         [ 0.8553, -0.0934,  0.0290,  ...,  0.0155, -0.0228, -0.2379],\n",
      "         [ 0.8468, -0.0868, -0.0270,  ...,  0.0681, -0.1890, -0.3481],\n",
      "         ...,\n",
      "         [-0.1675,  0.1178,  0.2676,  ...,  0.4275, -0.9559,  0.0071],\n",
      "         [ 0.0372,  0.4384, -0.0244,  ..., -0.0046, -0.1864, -0.0974],\n",
      "         [ 0.5809, -0.0199,  0.1729,  ...,  0.0371, -0.1984, -0.2354]]],\n",
      "       requires_grad=True)\n",
      "blocks.0.norm1.weight Parameter containing:\n",
      "tensor([0.6856, 0.9604, 0.8827, 0.6854, 1.0185, 0.8176, 1.0376, 0.9117, 0.7301,\n",
      "        0.7902, 0.8589, 1.0115, 0.9787, 0.8771, 0.8619, 0.8582, 0.8754, 0.6512,\n",
      "        0.9831, 0.9016, 0.7573, 0.7651, 0.8339, 0.7650, 1.0728, 0.9350, 1.0534,\n",
      "        0.7801, 0.8999, 0.7833, 0.8740, 0.9663], requires_grad=True)\n",
      "blocks.0.norm1.bias Parameter containing:\n",
      "tensor([-0.0687,  0.0462, -0.0070, -0.0710,  0.0167,  0.0221,  0.0440, -0.0049,\n",
      "         0.1194, -0.0653, -0.0037,  0.0572,  0.0040,  0.0495, -0.0584,  0.0381,\n",
      "         0.0223,  0.1085,  0.0178, -0.0560, -0.1219,  0.0423,  0.0324, -0.0434,\n",
      "         0.0368, -0.0292, -0.0417,  0.1079, -0.0275, -0.0756,  0.1365,  0.1726],\n",
      "       requires_grad=True)\n",
      "blocks.0.norm2.weight Parameter containing:\n",
      "tensor([0.8184, 0.9983, 1.1043, 0.9160, 0.9670, 0.7713, 0.8564, 0.9730, 1.0234,\n",
      "        0.9035, 0.8818, 0.8945, 1.1544, 0.9726, 1.0333, 0.9122, 0.9958, 0.8537,\n",
      "        1.0577, 0.9184, 0.8926, 1.1032, 0.9765, 0.8653, 1.0815, 0.7838, 0.7517,\n",
      "        1.1854, 0.8749, 0.9874, 0.9896, 0.8065], requires_grad=True)\n",
      "blocks.0.norm2.bias Parameter containing:\n",
      "tensor([-0.0364,  0.1332, -0.2950,  0.3259, -0.0309,  0.0678,  0.0121,  0.2357,\n",
      "        -0.0111,  0.0584,  0.2294, -0.1960, -0.0656,  0.1071,  0.1853, -0.0401,\n",
      "         0.0123, -0.1851, -0.2956, -0.0909,  0.1333,  0.0971,  0.1366,  0.0137,\n",
      "        -0.1360, -0.0249,  0.1060, -0.0447,  0.0408,  0.0012,  0.0659, -0.0461],\n",
      "       requires_grad=True)\n",
      "blocks.1.norm1.weight Parameter containing:\n",
      "tensor([0.8357, 0.8773, 0.8631, 1.0066, 0.9604, 0.9218, 0.9320, 0.9611, 0.9557,\n",
      "        0.7286, 0.9646, 1.0811, 0.9174, 0.9370, 0.8948, 0.7890, 0.8427, 0.7328,\n",
      "        0.9980, 0.8564, 0.8396, 1.0147, 0.8664, 0.7128, 0.6228, 0.9012, 1.0769,\n",
      "        1.1490, 0.9723, 1.0398, 0.7327, 0.7840], requires_grad=True)\n",
      "blocks.1.norm1.bias Parameter containing:\n",
      "tensor([-0.2183,  0.0331,  0.2205,  0.0952, -0.0131,  0.0621, -0.1138,  0.0659,\n",
      "         0.0041, -0.1003, -0.0337,  0.0887, -0.1654,  0.0047, -0.0347, -0.0934,\n",
      "         0.2148, -0.1302, -0.1355,  0.1319,  0.0074, -0.2160,  0.0936, -0.2007,\n",
      "         0.0290,  0.1571,  0.1697,  0.0816, -0.0267, -0.0289, -0.0825, -0.1751],\n",
      "       requires_grad=True)\n",
      "blocks.1.norm2.weight Parameter containing:\n",
      "tensor([0.9321, 0.9302, 0.9439, 0.9348, 0.8775, 0.9029, 0.9287, 0.9400, 1.1148,\n",
      "        1.0622, 1.2333, 1.0539, 0.8974, 0.8612, 1.0653, 1.2702, 0.9843, 0.7610,\n",
      "        1.0023, 1.0461, 0.9856, 1.0733, 0.9758, 0.9362, 1.1243, 0.9465, 1.0641,\n",
      "        0.9151, 1.1200, 1.1813, 0.9658, 0.9630], requires_grad=True)\n",
      "blocks.1.norm2.bias Parameter containing:\n",
      "tensor([ 0.1355, -0.0248, -0.0081, -0.0451,  0.0033,  0.0590,  0.0125,  0.0964,\n",
      "        -0.0471, -0.0195,  0.0603, -0.0253,  0.2019, -0.0319,  0.0813,  0.0635,\n",
      "         0.0977, -0.2376,  0.1477, -0.0608, -0.1167, -0.0630,  0.1093,  0.2239,\n",
      "        -0.0583, -0.1860,  0.0470,  0.0357,  0.0868, -0.1120,  0.0407,  0.1726],\n",
      "       requires_grad=True)\n",
      "norm.weight Parameter containing:\n",
      "tensor([1.7374, 1.4572, 1.4319, 1.2406, 1.4881, 1.6341, 1.4195, 1.5522, 1.5251,\n",
      "        1.7044, 1.4685, 1.6987, 1.3648, 1.6200, 1.4480, 1.0426, 1.4256, 1.2203,\n",
      "        1.5189, 1.4590, 1.3866, 1.4877, 1.4535, 1.8139, 1.3120, 1.4414, 1.4518,\n",
      "        1.3753, 1.6505, 1.6933, 1.5289, 1.3967], requires_grad=True)\n",
      "norm.bias Parameter containing:\n",
      "tensor([ 0.0889, -0.0463, -0.1263,  0.0929, -0.2152, -0.0829,  0.1460,  0.0734,\n",
      "         0.0771,  0.0896,  0.1180,  0.0713, -0.0076, -0.0016, -0.1070, -0.1246,\n",
      "        -0.0993,  0.0119, -0.0677, -0.0161, -0.0991, -0.0277,  0.0820, -0.1838,\n",
      "         0.0054,  0.0989,  0.2640,  0.2382,  0.0083, -0.0817, -0.0805, -0.0928],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for name, param in quantized_model.named_parameters():\n",
    "    print(name, param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model):\n",
    "    correct, total = 0, 0\n",
    "    model.eval()\n",
    "    # Setting the model in evaluation mode.\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            # Loading batch images and ground truth onto device\n",
    "            outputs = model(images)\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "    return f\"Accuracy on test set: {(100 * correct / total):.2f}%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Accuracy on test set: 95.30%'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(float_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Accuracy on test set: 70.79%'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(quantized_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " is not quantized.\n",
      "activation_post_process_0 is not quantized.\n",
      "activation_post_process_0.activation_post_process is not quantized.\n",
      "patch_embed is not quantized.\n",
      "activation_post_process_1 is not quantized.\n",
      "activation_post_process_1.activation_post_process is not quantized.\n",
      "activation_post_process_2 is not quantized.\n",
      "activation_post_process_2.activation_post_process is not quantized.\n",
      "activation_post_process_3 is not quantized.\n",
      "activation_post_process_3.activation_post_process is not quantized.\n",
      "activation_post_process_6 is not quantized.\n",
      "activation_post_process_6.activation_post_process is not quantized.\n",
      "activation_post_process_7 is not quantized.\n",
      "activation_post_process_7.activation_post_process is not quantized.\n",
      "activation_post_process_8 is not quantized.\n",
      "activation_post_process_8.activation_post_process is not quantized.\n",
      "blocks is not quantized.\n",
      "blocks.0 is not quantized.\n",
      "blocks.0.attn is not quantized.\n",
      "blocks.0.mlp is not quantized.\n",
      "blocks.1 is not quantized.\n",
      "blocks.1.attn is not quantized.\n",
      "blocks.1.mlp is not quantized.\n",
      "activation_post_process_9 is not quantized.\n",
      "activation_post_process_9.activation_post_process is not quantized.\n",
      "activation_post_process_10 is not quantized.\n",
      "activation_post_process_10.activation_post_process is not quantized.\n",
      "activation_post_process_13 is not quantized.\n",
      "activation_post_process_13.activation_post_process is not quantized.\n",
      "activation_post_process_15 is not quantized.\n",
      "activation_post_process_15.activation_post_process is not quantized.\n",
      "activation_post_process_17 is not quantized.\n",
      "activation_post_process_17.activation_post_process is not quantized.\n",
      "activation_post_process_18 is not quantized.\n",
      "activation_post_process_18.activation_post_process is not quantized.\n",
      "activation_post_process_19 is not quantized.\n",
      "activation_post_process_19.activation_post_process is not quantized.\n",
      "activation_post_process_21 is not quantized.\n",
      "activation_post_process_21.activation_post_process is not quantized.\n",
      "activation_post_process_22 is not quantized.\n",
      "activation_post_process_22.activation_post_process is not quantized.\n",
      "activation_post_process_23 is not quantized.\n",
      "activation_post_process_23.activation_post_process is not quantized.\n",
      "activation_post_process_24 is not quantized.\n",
      "activation_post_process_24.activation_post_process is not quantized.\n",
      "activation_post_process_25 is not quantized.\n",
      "activation_post_process_25.activation_post_process is not quantized.\n",
      "activation_post_process_26 is not quantized.\n",
      "activation_post_process_26.activation_post_process is not quantized.\n",
      "activation_post_process_27 is not quantized.\n",
      "activation_post_process_27.activation_post_process is not quantized.\n",
      "activation_post_process_28 is not quantized.\n",
      "activation_post_process_28.activation_post_process is not quantized.\n",
      "activation_post_process_29 is not quantized.\n",
      "activation_post_process_29.activation_post_process is not quantized.\n",
      "activation_post_process_30 is not quantized.\n",
      "activation_post_process_30.activation_post_process is not quantized.\n",
      "activation_post_process_31 is not quantized.\n",
      "activation_post_process_31.activation_post_process is not quantized.\n",
      "activation_post_process_32 is not quantized.\n",
      "activation_post_process_32.activation_post_process is not quantized.\n",
      "activation_post_process_33 is not quantized.\n",
      "activation_post_process_33.activation_post_process is not quantized.\n",
      "activation_post_process_36 is not quantized.\n",
      "activation_post_process_36.activation_post_process is not quantized.\n",
      "activation_post_process_38 is not quantized.\n",
      "activation_post_process_38.activation_post_process is not quantized.\n",
      "activation_post_process_40 is not quantized.\n",
      "activation_post_process_40.activation_post_process is not quantized.\n",
      "activation_post_process_41 is not quantized.\n",
      "activation_post_process_41.activation_post_process is not quantized.\n",
      "activation_post_process_42 is not quantized.\n",
      "activation_post_process_42.activation_post_process is not quantized.\n",
      "activation_post_process_44 is not quantized.\n",
      "activation_post_process_44.activation_post_process is not quantized.\n",
      "activation_post_process_45 is not quantized.\n",
      "activation_post_process_45.activation_post_process is not quantized.\n",
      "activation_post_process_46 is not quantized.\n",
      "activation_post_process_46.activation_post_process is not quantized.\n",
      "activation_post_process_47 is not quantized.\n",
      "activation_post_process_47.activation_post_process is not quantized.\n",
      "activation_post_process_48 is not quantized.\n",
      "activation_post_process_48.activation_post_process is not quantized.\n",
      "activation_post_process_49 is not quantized.\n",
      "activation_post_process_49.activation_post_process is not quantized.\n",
      "activation_post_process_50 is not quantized.\n",
      "activation_post_process_50.activation_post_process is not quantized.\n",
      "activation_post_process_51 is not quantized.\n",
      "activation_post_process_51.activation_post_process is not quantized.\n",
      "activation_post_process_52 is not quantized.\n",
      "activation_post_process_52.activation_post_process is not quantized.\n",
      "activation_post_process_53 is not quantized.\n",
      "activation_post_process_53.activation_post_process is not quantized.\n",
      "activation_post_process_54 is not quantized.\n",
      "activation_post_process_54.activation_post_process is not quantized.\n",
      "activation_post_process_55 is not quantized.\n",
      "activation_post_process_55.activation_post_process is not quantized.\n",
      "activation_post_process_56 is not quantized.\n",
      "activation_post_process_56.activation_post_process is not quantized.\n",
      "activation_post_process_57 is not quantized.\n",
      "activation_post_process_57.activation_post_process is not quantized.\n"
     ]
    }
   ],
   "source": [
    "for name, module in prepared_model.named_modules():\n",
    "    if not hasattr(module, 'qconfig'):\n",
    "        print(f\"{name} is not quantized.\")\n",
    "    # else:\n",
    "    #     print(f\"{name} is not quantized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         ...,\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float_model.state_dict()['pos_embed'] == quantized_model.state_dict()['pos_embed'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: activation_post_process_0\n",
      "Scale: 0.06666667014360428\n",
      "Zero Point: 0\n",
      "Layer: activation_post_process_1\n",
      "Scale: 0.3135177195072174\n",
      "Zero Point: 7\n",
      "Layer: activation_post_process_2\n",
      "Scale: 0.31371864676475525\n",
      "Zero Point: 7\n",
      "Layer: activation_post_process_3\n",
      "Scale: 0.18223203718662262\n",
      "Zero Point: 7\n",
      "Layer: activation_post_process_6\n",
      "Scale: 0.16426581144332886\n",
      "Zero Point: 7\n",
      "Layer: activation_post_process_7\n",
      "Scale: 0.27967262268066406\n",
      "Zero Point: 7\n",
      "Layer: activation_post_process_8\n",
      "Scale: 0.26970183849334717\n",
      "Zero Point: 7\n",
      "Layer: activation_post_process_9\n",
      "Scale: 0.43550142645835876\n",
      "Zero Point: 7\n",
      "Layer: activation_post_process_10\n",
      "Scale: 0.7412218451499939\n",
      "Zero Point: 7\n",
      "Layer: activation_post_process_13\n",
      "Scale: 0.7402080297470093\n",
      "Zero Point: 7\n",
      "Layer: activation_post_process_15\n",
      "Scale: 3.5287368297576904\n",
      "Zero Point: 7\n",
      "Layer: activation_post_process_17\n",
      "Scale: 0.06555232405662537\n",
      "Zero Point: 0\n",
      "Layer: activation_post_process_18\n",
      "Scale: 0.06545073539018631\n",
      "Zero Point: 0\n",
      "Layer: activation_post_process_19\n",
      "Scale: 0.3735204041004181\n",
      "Zero Point: 7\n",
      "Layer: activation_post_process_21\n",
      "Scale: 0.3668602705001831\n",
      "Zero Point: 7\n",
      "Layer: activation_post_process_22\n",
      "Scale: 0.566595196723938\n",
      "Zero Point: 8\n",
      "Layer: activation_post_process_23\n",
      "Scale: 0.5392560958862305\n",
      "Zero Point: 8\n",
      "Layer: activation_post_process_24\n",
      "Scale: 0.6024805903434753\n",
      "Zero Point: 8\n",
      "Layer: activation_post_process_25\n",
      "Scale: 0.5538877248764038\n",
      "Zero Point: 7\n",
      "Layer: activation_post_process_26\n",
      "Scale: 0.6495746970176697\n",
      "Zero Point: 8\n",
      "Layer: activation_post_process_27\n",
      "Scale: 0.29741886258125305\n",
      "Zero Point: 1\n",
      "Layer: activation_post_process_28\n",
      "Scale: 0.29027053713798523\n",
      "Zero Point: 1\n",
      "Layer: activation_post_process_29\n",
      "Scale: 0.47272351384162903\n",
      "Zero Point: 8\n",
      "Layer: activation_post_process_30\n",
      "Scale: 0.4639014005661011\n",
      "Zero Point: 8\n",
      "Layer: activation_post_process_31\n",
      "Scale: 0.6411187648773193\n",
      "Zero Point: 7\n",
      "Layer: activation_post_process_32\n",
      "Scale: 0.5236049294471741\n",
      "Zero Point: 7\n",
      "Layer: activation_post_process_33\n",
      "Scale: 0.6853338479995728\n",
      "Zero Point: 8\n",
      "Layer: activation_post_process_36\n",
      "Scale: 0.4545589089393616\n",
      "Zero Point: 8\n",
      "Layer: activation_post_process_38\n",
      "Scale: 1.8874266147613525\n",
      "Zero Point: 8\n",
      "Layer: activation_post_process_40\n",
      "Scale: 0.04091059789061546\n",
      "Zero Point: 0\n",
      "Layer: activation_post_process_41\n",
      "Scale: 0.03891216963529587\n",
      "Zero Point: 0\n",
      "Layer: activation_post_process_42\n",
      "Scale: 0.5059687495231628\n",
      "Zero Point: 8\n",
      "Layer: activation_post_process_44\n",
      "Scale: 0.49330267310142517\n",
      "Zero Point: 8\n",
      "Layer: activation_post_process_45\n",
      "Scale: 0.8142036199569702\n",
      "Zero Point: 7\n",
      "Layer: activation_post_process_46\n",
      "Scale: 0.786758303642273\n",
      "Zero Point: 7\n",
      "Layer: activation_post_process_47\n",
      "Scale: 0.9895721673965454\n",
      "Zero Point: 7\n",
      "Layer: activation_post_process_48\n",
      "Scale: 0.605212926864624\n",
      "Zero Point: 8\n",
      "Layer: activation_post_process_49\n",
      "Scale: 0.9100295901298523\n",
      "Zero Point: 7\n",
      "Layer: activation_post_process_50\n",
      "Scale: 0.47376710176467896\n",
      "Zero Point: 0\n",
      "Layer: activation_post_process_51\n",
      "Scale: 0.4564287066459656\n",
      "Zero Point: 0\n",
      "Layer: activation_post_process_52\n",
      "Scale: 0.7971881628036499\n",
      "Zero Point: 8\n",
      "Layer: activation_post_process_53\n",
      "Scale: 0.7716038823127747\n",
      "Zero Point: 8\n",
      "Layer: activation_post_process_54\n",
      "Scale: 1.2980501651763916\n",
      "Zero Point: 8\n",
      "Layer: activation_post_process_55\n",
      "Scale: 0.7963821887969971\n",
      "Zero Point: 7\n",
      "Layer: activation_post_process_56\n",
      "Scale: 0.6276390552520752\n",
      "Zero Point: 7\n",
      "Layer: activation_post_process_57\n",
      "Scale: 1.460162878036499\n",
      "Zero Point: 6\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Function to extract scale and zero point values from the prepared model\n",
    "def extract_quantization_params(model):\n",
    "    quantization_params = {}\n",
    "    for name, module in model.named_modules():\n",
    "        if hasattr(module, 'activation_post_process'):\n",
    "            observer = module.activation_post_process\n",
    "            if isinstance(observer, torch.quantization.observer.MovingAverageMinMaxObserver):\n",
    "                scale, zero_point = observer.calculate_qparams()\n",
    "                quantization_params[name] = {'scale': scale.item(), 'zero_point': zero_point.item()}\n",
    "    return quantization_params\n",
    "\n",
    "# Extract quantization parameters\n",
    "quantization_params = extract_quantization_params(prepared_model)\n",
    "\n",
    "# Print the scale and zero point values for each observer\n",
    "for name, params in quantization_params.items():\n",
    "    print(f\"Layer: {name}\")\n",
    "    print(f\"Scale: {params['scale']}\")\n",
    "    print(f\"Zero Point: {params['zero_point']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
