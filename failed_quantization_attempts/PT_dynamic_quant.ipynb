{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\finetuner\\My_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision\n",
    "from Vision_transformer import VisionTransformer, VisionTransformerForPTQ, CustomDataset\n",
    "import torchvision.transforms as transforms\n",
    "from pprint import pprint\n",
    "from torchsummary import summary\n",
    "import json\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function torch.ao.quantization.quantize.quantize_dynamic(model, qconfig_spec=None, dtype=torch.qint8, mapping=None, inplace=False)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ao.quantization.quantize_dynamic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cpu\")\n",
    "# We don't want to perform our quantization step on cuda GPU. It is not supported.\n",
    "with open('config.json') as f:\n",
    "    custom_config = json.load(f)\n",
    "# Custom configurations for the VisionTransformer.\n",
    "# Transformer can be customized with these configurations.\n",
    "# Refer to documentation of the class VisionTransformer\n",
    "# (`VisionTransformer.__doc__`, use pprint for cleaner display)\n",
    "# for exact details of the customization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load saved model\n",
    "MNIST_ViT = VisionTransformer(**custom_config).to(device=device)\n",
    "checkpoint = torch.load(\"model.pth\")\n",
    "MNIST_ViT.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])   # Transform object to apply on the dataset.\n",
    "\n",
    "# train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "# Loading/Downloading dataset. `download` can be `False` if the data is present in the root directory\n",
    "# Else it will download the dataset to to the root location.\n",
    "\n",
    "test_ds = CustomDataset(data=test_dataset, device=device)\n",
    "# Made custom dataset objects from the MNIST dataset.\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_ds, batch_size=64, shuffle=False)\n",
    "# DataLoaders for fast implementation of loading batch-wise data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model : VisionTransformer):\n",
    "    correct, total = 0, 0\n",
    "    model.eval()\n",
    "    # Setting the model in evaluation mode.\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            # Loading batch images and ground truth onto device\n",
    "            outputs = model(images)\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "    return f\"Accuracy on test set: {(100 * correct / total):.2f}%\"\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Accuracy on test set: 49.74%'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(MNIST_ViT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights before quantization\n",
      "Parameter containing:\n",
      "tensor([[-7.0662e-02,  1.3499e-01, -3.8009e-01,  6.0584e-01,  4.1917e-01,\n",
      "         -3.7075e-01,  2.0450e-01, -4.0412e-01,  6.2776e-02, -4.6475e-01,\n",
      "         -5.9376e-01, -2.4132e-01, -4.2393e-01,  1.8389e-01,  1.8460e-01,\n",
      "          2.1956e-01, -3.1886e-01, -3.3674e-01,  8.8356e-02, -4.2352e-01,\n",
      "         -4.0939e-02, -1.0542e-01,  1.8793e-01, -7.2534e-02,  1.6244e-01,\n",
      "         -1.2656e-01,  3.3465e-01,  8.5337e-02,  5.2124e-01, -3.7563e-01,\n",
      "         -4.5943e-01,  1.7528e-01],\n",
      "        [ 3.0352e-02, -5.3761e-01,  5.1218e-01,  5.0710e-01, -1.9978e-01,\n",
      "          1.8169e-01,  2.8051e-01, -2.8327e-04,  1.3403e-01,  3.0515e-01,\n",
      "         -1.6733e-01, -1.4994e-01, -2.9695e-01, -4.3653e-02, -2.0136e-01,\n",
      "         -3.6033e-01,  3.7734e-01,  7.5984e-01, -3.7332e-01,  2.5667e-01,\n",
      "         -2.6232e-01, -1.7522e-01, -3.5150e-01, -4.4539e-02,  3.4139e-02,\n",
      "          2.3827e-01,  3.2483e-01, -4.8200e-01, -3.3107e-01,  6.6220e-04,\n",
      "         -1.3452e-01, -4.9982e-03],\n",
      "        [-1.8054e-01, -3.1714e-01,  4.3834e-02, -1.5373e-01,  4.0802e-01,\n",
      "          3.0738e-01, -1.2508e-01,  2.7700e-01,  4.3899e-01,  5.5846e-01,\n",
      "          2.1806e-01, -9.6559e-02, -2.9435e-01,  8.5169e-02,  3.8948e-01,\n",
      "         -3.4951e-01,  2.4702e-01, -1.7520e-01, -2.9415e-03, -4.1315e-02,\n",
      "          2.3549e-01,  2.4263e-01, -5.3219e-01,  4.3015e-01,  3.5472e-01,\n",
      "         -1.2378e-01,  3.5077e-01, -2.0200e-01,  3.4359e-01, -6.6013e-01,\n",
      "         -5.8778e-01, -1.6139e-01],\n",
      "        [-2.9198e-01, -2.7329e-01,  1.5756e-01, -2.4867e-01,  8.9417e-02,\n",
      "          2.3414e-01, -2.4737e-01,  2.3148e-01,  2.1705e-01, -3.7169e-01,\n",
      "          2.6820e-01,  4.5165e-01,  4.8011e-01, -2.7147e-01,  3.6934e-01,\n",
      "         -4.0749e-01, -2.0289e-01, -5.9938e-02, -2.7835e-01, -1.2312e-01,\n",
      "          2.2346e-01, -4.6082e-01,  3.0568e-01,  3.6012e-01, -3.5758e-01,\n",
      "          1.2761e-01,  2.2901e-01, -2.3909e-01, -2.1400e-01,  3.9785e-02,\n",
      "          3.6608e-01, -2.2432e-01],\n",
      "        [ 3.1808e-01,  1.6361e-01, -5.3151e-01, -5.7490e-02, -3.4209e-01,\n",
      "         -7.1168e-02,  5.5124e-01, -3.6624e-01, -2.8539e-01,  2.5833e-01,\n",
      "          6.3101e-02,  1.8964e-01,  3.0680e-01,  2.6596e-01, -2.7185e-01,\n",
      "          3.1318e-01,  4.3693e-01,  3.1748e-01, -3.4351e-02, -5.3779e-02,\n",
      "         -4.4899e-01,  1.4710e-01, -7.2955e-02, -2.7403e-01, -3.3528e-01,\n",
      "         -3.3355e-01, -3.8520e-01,  3.3776e-01, -6.9318e-02,  1.5690e-01,\n",
      "         -5.4700e-01, -4.0055e-01],\n",
      "        [-3.8485e-01,  3.2392e-01,  2.1606e-01, -3.4132e-01, -5.2945e-02,\n",
      "         -5.6807e-01, -2.9394e-01,  2.3735e-01, -1.6277e-01, -3.1090e-01,\n",
      "         -4.7150e-01, -3.2620e-01,  8.1708e-02, -4.2242e-01, -3.4786e-03,\n",
      "          1.9497e-01, -3.2167e-01, -1.3660e-01, -8.5525e-02,  2.2041e-01,\n",
      "          3.6731e-01, -4.0298e-01,  1.4728e-01,  2.2276e-01,  1.1586e-01,\n",
      "          1.2572e-01, -5.0553e-02,  5.2880e-01, -3.1495e-01,  1.2935e-01,\n",
      "          4.0558e-01,  3.3001e-01],\n",
      "        [-2.7471e-01,  3.1913e-01,  2.7833e-01,  2.6196e-01,  3.4191e-01,\n",
      "         -5.6097e-01,  3.9589e-01, -2.6129e-02,  2.1695e-01,  9.3651e-02,\n",
      "         -6.9874e-01, -3.6346e-01, -3.9845e-01,  5.5253e-01,  1.0878e-01,\n",
      "          2.4859e-01,  3.0128e-01, -1.4754e-01, -2.5752e-01,  6.2928e-01,\n",
      "         -1.2901e-02, -1.0364e-01,  2.1258e-01,  1.9568e-01,  3.0191e-01,\n",
      "         -4.2708e-01, -5.5330e-01, -1.3913e-01,  5.6179e-01,  1.1965e-01,\n",
      "          1.4981e-02, -1.4524e-01],\n",
      "        [ 6.8941e-01, -2.3531e-01, -1.9712e-01, -2.2538e-02, -2.9447e-01,\n",
      "          2.4514e-01, -2.2437e-02,  3.3288e-01,  4.3496e-01, -4.7282e-02,\n",
      "          3.1498e-01,  4.1810e-01, -6.2002e-03, -1.7665e-01, -5.3883e-01,\n",
      "         -3.0075e-01,  5.1350e-01, -1.6667e-01,  3.8576e-01, -3.5322e-01,\n",
      "          2.3290e-01,  3.5336e-01, -5.1978e-01, -3.4039e-01, -4.4580e-01,\n",
      "          2.7498e-01,  2.7252e-01, -2.1813e-01, -8.3805e-02, -4.2938e-01,\n",
      "          1.6631e-01,  2.2453e-01],\n",
      "        [-2.0935e-01,  7.5188e-02, -1.6132e-02, -2.5073e-01,  3.9894e-01,\n",
      "          1.8144e-01, -3.3273e-01, -2.6652e-01, -1.4211e-01,  3.2640e-01,\n",
      "          2.1024e-01, -3.3081e-01,  2.3006e-01,  3.0565e-01, -3.5059e-01,\n",
      "          3.4438e-02, -2.6455e-01, -3.5111e-01,  3.1036e-01,  6.9776e-02,\n",
      "         -3.1493e-01, -1.5650e-01,  7.8121e-02, -2.2023e-01,  3.2235e-01,\n",
      "          5.1366e-02,  1.8662e-01, -2.7377e-01, -1.4956e-01,  1.8010e-01,\n",
      "          3.3583e-02, -1.8554e-01],\n",
      "        [ 3.4102e-01,  2.5776e-01, -1.2326e-01, -1.3406e-01, -2.0269e-01,\n",
      "          8.1846e-02,  2.9824e-01, -3.0876e-01, -3.2759e-01,  1.4421e-01,\n",
      "          2.4605e-01,  1.5572e-01,  8.5378e-02, -3.1285e-01,  1.3058e-01,\n",
      "          1.4342e-01, -2.2370e-01, -6.7050e-02,  3.7010e-01, -3.9500e-01,\n",
      "          1.1542e-02,  2.5621e-01,  2.8539e-01, -1.1901e-01, -4.4891e-01,\n",
      "         -2.8812e-01, -2.7159e-01, -1.0295e-01, -2.0197e-01,  3.1497e-01,\n",
      "          3.1011e-01,  3.1235e-01]], requires_grad=True)\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "# Weights matrix of the model before quantization\n",
    "print('Weights before quantization')\n",
    "print(MNIST_ViT.head.weight)\n",
    "print(MNIST_ViT.head.weight.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the model before quantization\n",
      "Size (KB): 63.01\n"
     ]
    }
   ],
   "source": [
    "def print_size_of_model(model):\n",
    "    torch.save(model.state_dict(), \"temp_delme.p\")\n",
    "    print('Size (KB):', os.path.getsize(\"temp_delme.p\")/1e3)\n",
    "    os.remove('temp_delme.p')\n",
    "\n",
    "print('Size of the model before quantization')\n",
    "print_size_of_model(MNIST_ViT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model before quantization: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Accuracy on test set: 49.74%'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'Accuracy of the model before quantization: ')\n",
    "test(MNIST_ViT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading weights to the object that we have to quantize\n",
    "net_quantized = VisionTransformerForPTQ(**custom_config).to(device=device)\n",
    "net_quantized.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_quantized.eval()\n",
    "max_bit_length = 4\n",
    "# net_quantized.qconfig = torch.ao.quantization.default_qconfig\n",
    "\n",
    "net_quantized.qconfig = torch.quantization.QConfig(\n",
    "    activation=torch.quantization.fake_quantize.FakeQuantize.with_args(observer = torch.quantization.observer.MovingAverageMinMaxObserver.with_args(dtype=torch.quint8), quant_min = 0 ,quant_max=2**(max_bit_length)-1, dtype=torch.quint8), \n",
    "    weight=torch.quantization.fake_quantize.FakeQuantize.with_args(observer = torch.quantization.observer.MovingAverageMinMaxObserver.with_args(dtype=torch.qint8), quant_min = 0 ,quant_max=2**(max_bit_length)-1, dtype=torch.qint8)\n",
    ")\n",
    "\n",
    "# net_quantized.qconfig = torch.ao.quantization.QConfig(\n",
    "#     activation=torch.ao.quantization.fake_quantize.FakeQuantize.with_args(observer = torch.ao.quantization.observer.MovingAverageMinMaxObserver.with_args(dtype=torch.quint8), quant_min =-2**(max_bit_length-1) ,quant_max=2**(max_bit_length-1)-1, dtype=torch.quint8), \n",
    "#     weight=torch.ao.quantization.fake_quantize.FakeQuantize.with_args(observer = torch.ao.quantization.observer.MovingAverageMinMaxObserver.with_args(dtype=torch.quint8), quant_min =-2**(max_bit_length-1) ,quant_max=2**(max_bit_length-1)-1, dtype=torch.quint8)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FakeQuantize'>, observer=functools.partial(<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, dtype=torch.quint8){}, quant_min=0, quant_max=15, dtype=torch.quint8){}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FakeQuantize'>, observer=functools.partial(<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, dtype=torch.qint8){}, quant_min=0, quant_max=15, dtype=torch.qint8){})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_quantized.qconfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.ao.quantization.QConfig(\n",
    "#     activation=torch.ao.quantization.fake_quantize.FakeQuantize.with_args(quant_min =-2**(max_bit_length-1) ,quant_max=2**(max_bit_length-1)-1, dtype=torch.qint8), \n",
    "#     weight=torch.ao.quantization.fake_quantize.FakeQuantize.with_args(quant_min =-2**(max_bit_length-1) ,quant_max=2**(max_bit_length-1)-1, dtype=torch.qint8)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformerForPTQ(\n",
       "  (quant): QuantStub(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(\n",
       "      1, 32, kernel_size=(4, 4), stride=(4, 4)\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.2, inplace=False)\n",
       "  (blocks): ModuleList(\n",
       "    (0-1): 2 x Block(\n",
       "      (norm1): LayerNorm(\n",
       "        (32,), eps=1e-07, elementwise_affine=True\n",
       "        (activation_post_process): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32)\n",
       "          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "      )\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(\n",
       "          in_features=32, out_features=96, bias=True\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (attn_drop): Dropout(p=0.2, inplace=False)\n",
       "        (proj): Linear(\n",
       "          in_features=32, out_features=32, bias=True\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (proj_drop): Dropout(p=0.2, inplace=False)\n",
       "        (operation): FloatFunctional(\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (sm): Softmax(dim=-1)\n",
       "      )\n",
       "      (norm2): LayerNorm(\n",
       "        (32,), eps=1e-07, elementwise_affine=True\n",
       "        (activation_post_process): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32)\n",
       "          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(\n",
       "          in_features=32, out_features=12, bias=True\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(\n",
       "          in_features=12, out_features=32, bias=True\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (operations): FloatFunctional(\n",
       "        (activation_post_process): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32)\n",
       "          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm(\n",
       "    (32,), eps=1e-06, elementwise_affine=True\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (head): Linear(\n",
       "    in_features=32, out_features=10, bias=True\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (dequant): DeQuantStub()\n",
       "  (operation): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_quantized = torch.ao.quantization.prepare(net_quantized) # Insert observers\n",
    "net_quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Accuracy on test set: 15.72%'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(net_quantized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check statistics of the various layers\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VisionTransformerForPTQ(\n",
       "  (quant): QuantStub(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0840]), zero_point=tensor([6], dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5092379450798035, max_val=0.750572919845581)\n",
       "    )\n",
       "  )\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(\n",
       "      1, 32, kernel_size=(4, 4), stride=(4, 4)\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2216]), zero_point=tensor([7], dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.6397794485092163, max_val=1.6843312978744507)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.2, inplace=False)\n",
       "  (blocks): ModuleList(\n",
       "    (0): Block(\n",
       "      (norm1): LayerNorm(\n",
       "        (32,), eps=1e-07, elementwise_affine=True\n",
       "        (activation_post_process): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4167]), zero_point=tensor([7], dtype=torch.int32)\n",
       "          (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.9784739017486572, max_val=3.271495819091797)\n",
       "        )\n",
       "      )\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(\n",
       "          in_features=32, out_features=96, bias=True\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.6340]), zero_point=tensor([8], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.81951379776001, max_val=4.690359115600586)\n",
       "          )\n",
       "        )\n",
       "        (attn_drop): Dropout(p=0.2, inplace=False)\n",
       "        (proj): Linear(\n",
       "          in_features=32, out_features=32, bias=True\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4078]), zero_point=tensor([7], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.8778085708618164, max_val=3.23952054977417)\n",
       "          )\n",
       "        )\n",
       "        (proj_drop): Dropout(p=0.2, inplace=False)\n",
       "        (operation): FloatFunctional(\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([3.1262]), zero_point=tensor([7], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=-22.862285614013672, max_val=24.03031349182129)\n",
       "          )\n",
       "        )\n",
       "        (sm): Softmax(dim=-1)\n",
       "      )\n",
       "      (norm2): LayerNorm(\n",
       "        (32,), eps=1e-07, elementwise_affine=True\n",
       "        (activation_post_process): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.5531]), zero_point=tensor([7], dtype=torch.int32)\n",
       "          (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.002194881439209, max_val=4.293577194213867)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(\n",
       "          in_features=32, out_features=12, bias=True\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(\n",
       "          in_features=12, out_features=32, bias=True\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (operations): FloatFunctional(\n",
       "        (activation_post_process): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.5105]), zero_point=tensor([7], dtype=torch.int32)\n",
       "          (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.6159255504608154, max_val=4.042269229888916)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): Block(\n",
       "      (norm1): LayerNorm(\n",
       "        (32,), eps=1e-07, elementwise_affine=True\n",
       "        (activation_post_process): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.5035]), zero_point=tensor([7], dtype=torch.int32)\n",
       "          (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.697044849395752, max_val=3.854778289794922)\n",
       "        )\n",
       "      )\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(\n",
       "          in_features=32, out_features=96, bias=True\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.6426]), zero_point=tensor([7], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.733896255493164, max_val=4.904927730560303)\n",
       "          )\n",
       "        )\n",
       "        (attn_drop): Dropout(p=0.2, inplace=False)\n",
       "        (proj): Linear(\n",
       "          in_features=32, out_features=32, bias=True\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.6743]), zero_point=tensor([7], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.802699565887451, max_val=5.31251335144043)\n",
       "          )\n",
       "        )\n",
       "        (proj_drop): Dropout(p=0.2, inplace=False)\n",
       "        (operation): FloatFunctional(\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.5636]), zero_point=tensor([7], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=-10.784411430358887, max_val=12.66901969909668)\n",
       "          )\n",
       "        )\n",
       "        (sm): Softmax(dim=-1)\n",
       "      )\n",
       "      (norm2): LayerNorm(\n",
       "        (32,), eps=1e-07, elementwise_affine=True\n",
       "        (activation_post_process): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4828]), zero_point=tensor([7], dtype=torch.int32)\n",
       "          (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.391354560852051, max_val=3.8499205112457275)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(\n",
       "          in_features=32, out_features=12, bias=True\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(\n",
       "          in_features=12, out_features=32, bias=True\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (operations): FloatFunctional(\n",
       "        (activation_post_process): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.9244]), zero_point=tensor([8], dtype=torch.int32)\n",
       "          (activation_post_process): MovingAverageMinMaxObserver(min_val=-7.067206382751465, max_val=6.799321174621582)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm(\n",
       "    (32,), eps=1e-06, elementwise_affine=True\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.5917]), zero_point=tensor([7], dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.269731521606445, max_val=4.605199813842773)\n",
       "    )\n",
       "  )\n",
       "  (head): Linear(\n",
       "    in_features=32, out_features=10, bias=True\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.3499]), zero_point=tensor([8], dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-11.098151206970215, max_val=9.15041732788086)\n",
       "    )\n",
       "  )\n",
       "  (dequant): DeQuantStub()\n",
       "  (operation): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2420]), zero_point=tensor([7], dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.7341481447219849, max_val=1.8956173658370972)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'Check statistics of the various layers')\n",
    "net_quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformerForPTQ(\n",
       "  (quant): QuantStub(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0840]), zero_point=tensor([6], dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5092379450798035, max_val=0.750572919845581)\n",
       "    )\n",
       "  )\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(\n",
       "      1, 32, kernel_size=(4, 4), stride=(4, 4)\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2216]), zero_point=tensor([7], dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.6397794485092163, max_val=1.6843312978744507)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.2, inplace=False)\n",
       "  (blocks): ModuleList(\n",
       "    (0): Block(\n",
       "      (norm1): LayerNorm(\n",
       "        (32,), eps=1e-07, elementwise_affine=True\n",
       "        (activation_post_process): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4167]), zero_point=tensor([7], dtype=torch.int32)\n",
       "          (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.9784739017486572, max_val=3.271495819091797)\n",
       "        )\n",
       "      )\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(\n",
       "          in_features=32, out_features=96, bias=True\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.6340]), zero_point=tensor([8], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.81951379776001, max_val=4.690359115600586)\n",
       "          )\n",
       "        )\n",
       "        (attn_drop): Dropout(p=0.2, inplace=False)\n",
       "        (proj): Linear(\n",
       "          in_features=32, out_features=32, bias=True\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4078]), zero_point=tensor([7], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.8778085708618164, max_val=3.23952054977417)\n",
       "          )\n",
       "        )\n",
       "        (proj_drop): Dropout(p=0.2, inplace=False)\n",
       "        (operation): FloatFunctional(\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([3.1262]), zero_point=tensor([7], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=-22.862285614013672, max_val=24.03031349182129)\n",
       "          )\n",
       "        )\n",
       "        (sm): Softmax(dim=-1)\n",
       "      )\n",
       "      (norm2): LayerNorm(\n",
       "        (32,), eps=1e-07, elementwise_affine=True\n",
       "        (activation_post_process): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.5531]), zero_point=tensor([7], dtype=torch.int32)\n",
       "          (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.002194881439209, max_val=4.293577194213867)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(\n",
       "          in_features=32, out_features=12, bias=True\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(\n",
       "          in_features=12, out_features=32, bias=True\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (operations): FloatFunctional(\n",
       "        (activation_post_process): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.5105]), zero_point=tensor([7], dtype=torch.int32)\n",
       "          (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.6159255504608154, max_val=4.042269229888916)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): Block(\n",
       "      (norm1): LayerNorm(\n",
       "        (32,), eps=1e-07, elementwise_affine=True\n",
       "        (activation_post_process): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.5035]), zero_point=tensor([7], dtype=torch.int32)\n",
       "          (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.697044849395752, max_val=3.854778289794922)\n",
       "        )\n",
       "      )\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(\n",
       "          in_features=32, out_features=96, bias=True\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.6426]), zero_point=tensor([7], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.733896255493164, max_val=4.904927730560303)\n",
       "          )\n",
       "        )\n",
       "        (attn_drop): Dropout(p=0.2, inplace=False)\n",
       "        (proj): Linear(\n",
       "          in_features=32, out_features=32, bias=True\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.6743]), zero_point=tensor([7], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.802699565887451, max_val=5.31251335144043)\n",
       "          )\n",
       "        )\n",
       "        (proj_drop): Dropout(p=0.2, inplace=False)\n",
       "        (operation): FloatFunctional(\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.5636]), zero_point=tensor([7], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=-10.784411430358887, max_val=12.66901969909668)\n",
       "          )\n",
       "        )\n",
       "        (sm): Softmax(dim=-1)\n",
       "      )\n",
       "      (norm2): LayerNorm(\n",
       "        (32,), eps=1e-07, elementwise_affine=True\n",
       "        (activation_post_process): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4828]), zero_point=tensor([7], dtype=torch.int32)\n",
       "          (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.391354560852051, max_val=3.8499205112457275)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(\n",
       "          in_features=32, out_features=12, bias=True\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(\n",
       "          in_features=12, out_features=32, bias=True\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (operations): FloatFunctional(\n",
       "        (activation_post_process): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.9244]), zero_point=tensor([8], dtype=torch.int32)\n",
       "          (activation_post_process): MovingAverageMinMaxObserver(min_val=-7.067206382751465, max_val=6.799321174621582)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm(\n",
       "    (32,), eps=1e-06, elementwise_affine=True\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.5917]), zero_point=tensor([7], dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.269731521606445, max_val=4.605199813842773)\n",
       "    )\n",
       "  )\n",
       "  (head): Linear(\n",
       "    in_features=32, out_features=10, bias=True\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.3499]), zero_point=tensor([8], dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-11.098151206970215, max_val=9.15041732788086)\n",
       "    )\n",
       "  )\n",
       "  (dequant): DeQuantStub()\n",
       "  (operation): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2420]), zero_point=tensor([7], dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.7341481447219849, max_val=1.8956173658370972)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_quantized.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\finetuner\\My_env\\lib\\site-packages\\torch\\ao\\quantization\\utils.py:339: UserWarning: must run observer before calling calculate_qparams. Returning default values.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "net_quantized = torch.quantization.convert(net_quantized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check statistics of the various layers\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VisionTransformerForPTQ(\n",
       "  (quant): Quantize(scale=tensor([0.0840]), zero_point=tensor([6]), dtype=torch.quint8)\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): QuantizedConv2d(1, 32, kernel_size=(4, 4), stride=(4, 4), scale=0.22160738706588745, zero_point=7)\n",
       "  )\n",
       "  (pos_drop): QuantizedDropout(p=0.2, inplace=False)\n",
       "  (blocks): ModuleList(\n",
       "    (0): Block(\n",
       "      (norm1): QuantizedLayerNorm((32,), eps=1e-07, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): QuantizedLinear(in_features=32, out_features=96, scale=0.6339914798736572, zero_point=8, qscheme=torch.per_tensor_affine)\n",
       "        (attn_drop): QuantizedDropout(p=0.2, inplace=False)\n",
       "        (proj): QuantizedLinear(in_features=32, out_features=32, scale=0.4078219532966614, zero_point=7, qscheme=torch.per_tensor_affine)\n",
       "        (proj_drop): QuantizedDropout(p=0.2, inplace=False)\n",
       "        (operation): QFunctional(\n",
       "          scale=3.126173496246338, zero_point=7\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "        (sm): Softmax(dim=-1)\n",
       "      )\n",
       "      (norm2): QuantizedLayerNorm((32,), eps=1e-07, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (fc1): QuantizedLinear(in_features=32, out_features=12, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): QuantizedLinear(in_features=12, out_features=32, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (drop): QuantizedDropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (operations): QFunctional(\n",
       "        scale=0.5105463266372681, zero_point=7\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (1): Block(\n",
       "      (norm1): QuantizedLayerNorm((32,), eps=1e-07, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): QuantizedLinear(in_features=32, out_features=96, scale=0.6425883173942566, zero_point=7, qscheme=torch.per_tensor_affine)\n",
       "        (attn_drop): QuantizedDropout(p=0.2, inplace=False)\n",
       "        (proj): QuantizedLinear(in_features=32, out_features=32, scale=0.6743475794792175, zero_point=7, qscheme=torch.per_tensor_affine)\n",
       "        (proj_drop): QuantizedDropout(p=0.2, inplace=False)\n",
       "        (operation): QFunctional(\n",
       "          scale=1.563562035560608, zero_point=7\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "        (sm): Softmax(dim=-1)\n",
       "      )\n",
       "      (norm2): QuantizedLayerNorm((32,), eps=1e-07, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (fc1): QuantizedLinear(in_features=32, out_features=12, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): QuantizedLinear(in_features=12, out_features=32, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
       "        (drop): QuantizedDropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (operations): QFunctional(\n",
       "        scale=0.9244351983070374, zero_point=8\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): QuantizedLayerNorm((32,), eps=1e-06, elementwise_affine=True)\n",
       "  (head): QuantizedLinear(in_features=32, out_features=10, scale=1.3499046564102173, zero_point=8, qscheme=torch.per_tensor_affine)\n",
       "  (dequant): DeQuantize()\n",
       "  (operation): QFunctional(\n",
       "    scale=0.24198436737060547, zero_point=7\n",
       "    (activation_post_process): Identity()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'Check statistics of the various layers')\n",
    "net_quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights after quantization\n",
      "QuantizedLinear(in_features=32, out_features=10, scale=1.3499046564102173, zero_point=8, qscheme=torch.per_tensor_affine)\n"
     ]
    }
   ],
   "source": [
    "# Print the weights matrix of the model after quantization\n",
    "print('Weights after quantization')\n",
    "print(net_quantized.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the model after quantization\n",
      "Size (KB): 44.77\n"
     ]
    }
   ],
   "source": [
    "print('Size of the model after quantization')\n",
    "print_size_of_model(net_quantized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the model after quantization\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Could not run 'aten::_softmax.out' with arguments from the 'QuantizedCPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::_softmax.out' is only available for these backends: [CPU, CUDA, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nCPU: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen\\RegisterCPU.cpp:31357 [kernel]\nCUDA: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen\\RegisterCUDA.cpp:44411 [kernel]\nMeta: registered at /dev/null:228 [kernel]\nBackendSelect: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\core\\BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:154 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\functorch\\DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen\\RegisterFunctionalization_1.cpp:25069 [kernel]\nNamed: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\core\\NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\ADInplaceOrViewType_1.cpp:5216 [kernel]\nAutogradOther: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:19039 [autograd kernel]\nAutogradCPU: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:19039 [autograd kernel]\nAutogradCUDA: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:19039 [autograd kernel]\nAutogradHIP: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:19039 [autograd kernel]\nAutogradXLA: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:19039 [autograd kernel]\nAutogradMPS: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:19039 [autograd kernel]\nAutogradIPU: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:19039 [autograd kernel]\nAutogradXPU: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:19039 [autograd kernel]\nAutogradHPU: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:19039 [autograd kernel]\nAutogradVE: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:19039 [autograd kernel]\nAutogradLazy: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:19039 [autograd kernel]\nAutogradMTIA: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:19039 [autograd kernel]\nAutogradPrivateUse1: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:19039 [autograd kernel]\nAutogradPrivateUse2: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:19039 [autograd kernel]\nAutogradPrivateUse3: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:19039 [autograd kernel]\nAutogradMeta: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:19039 [autograd kernel]\nAutogradNestedTensor: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:19039 [autograd kernel]\nTracer: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\TraceType_3.cpp:14672 [kernel]\nAutocastCPU: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\autocast_mode.cpp:378 [backend fallback]\nAutocastCUDA: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\autocast_mode.cpp:244 [backend fallback]\nFuncTorchBatched: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\functorch\\LegacyBatchingRegistrations.cpp:720 [backend fallback]\nBatchedNestedTensor: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\functorch\\LegacyBatchingRegistrations.cpp:746 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\functorch\\VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\functorch\\TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:162 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\functorch\\DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:166 [backend fallback]\nPythonDispatcher: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:158 [backend fallback]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTesting the model after quantization\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet_quantized\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 9\u001b[0m, in \u001b[0;36mtest\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m      7\u001b[0m images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Loading batch images and ground truth onto device\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     12\u001b[0m total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32md:\\Projects\\finetuner\\My_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Projects\\finetuner\\My_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Projects\\finetuner\\VisionTransformer\\Vision_transformer.py:552\u001b[0m, in \u001b[0;36mVisionTransformerForPTQ.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    550\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_drop(x)\n\u001b[0;32m    551\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[1;32m--> 552\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    554\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n\u001b[0;32m    556\u001b[0m cls_token_final \u001b[38;5;241m=\u001b[39m x[:, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;66;03m#just the cls token\u001b[39;00m\n",
      "File \u001b[1;32md:\\Projects\\finetuner\\My_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Projects\\finetuner\\My_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Projects\\finetuner\\VisionTransformer\\Vision_transformer.py:281\u001b[0m, in \u001b[0;36mBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m    269\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Run forward pass.\u001b[39;00m\n\u001b[0;32m    270\u001b[0m \n\u001b[0;32m    271\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;124;03m        Shape `(n_samples, n_patches + 1, dim)`\u001b[39;00m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 281\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moperations\u001b[38;5;241m.\u001b[39madd(x, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    282\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moperations\u001b[38;5;241m.\u001b[39madd(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x)))\n\u001b[0;32m    284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32md:\\Projects\\finetuner\\My_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Projects\\finetuner\\My_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Projects\\finetuner\\VisionTransformer\\Vision_transformer.py:144\u001b[0m, in \u001b[0;36mAttention.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    141\u001b[0m dp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moperation\u001b[38;5;241m.\u001b[39mmul_scalar(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moperation\u001b[38;5;241m.\u001b[39mmatmul(q, k_t), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale)\n\u001b[0;32m    142\u001b[0m \u001b[38;5;66;03m# attn = dp.softmax(dim = -1) # (n_samples, n_heads, n_patches + 1, n_patches + 1)\u001b[39;00m\n\u001b[1;32m--> 144\u001b[0m attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    145\u001b[0m attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_drop(attn)\n\u001b[0;32m    147\u001b[0m \u001b[38;5;66;03m# weighted_avg = attn @ v # (n_samples, n_heads, n_patches + 1, head_dim)\u001b[39;00m\n",
      "File \u001b[1;32md:\\Projects\\finetuner\\My_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Projects\\finetuner\\My_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Projects\\finetuner\\My_env\\lib\\site-packages\\torch\\nn\\modules\\activation.py:1514\u001b[0m, in \u001b[0;36mSoftmax.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m   1513\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacklevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Projects\\finetuner\\My_env\\lib\\site-packages\\torch\\nn\\functional.py:1858\u001b[0m, in \u001b[0;36msoftmax\u001b[1;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[0;32m   1856\u001b[0m     dim \u001b[38;5;241m=\u001b[39m _get_softmax_dim(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim(), _stacklevel)\n\u001b[0;32m   1857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1858\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1860\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(dim, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: Could not run 'aten::_softmax.out' with arguments from the 'QuantizedCPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::_softmax.out' is only available for these backends: [CPU, CUDA, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nCPU: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen\\RegisterCPU.cpp:31357 [kernel]\nCUDA: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen\\RegisterCUDA.cpp:44411 [kernel]\nMeta: registered at /dev/null:228 [kernel]\nBackendSelect: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\core\\BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:154 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\functorch\\DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen\\RegisterFunctionalization_1.cpp:25069 [kernel]\nNamed: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\core\\NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\ADInplaceOrViewType_1.cpp:5216 [kernel]\nAutogradOther: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:19039 [autograd kernel]\nAutogradCPU: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:19039 [autograd kernel]\nAutogradCUDA: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:19039 [autograd kernel]\nAutogradHIP: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:19039 [autograd kernel]\nAutogradXLA: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:19039 [autograd kernel]\nAutogradMPS: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:19039 [autograd kernel]\nAutogradIPU: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:19039 [autograd kernel]\nAutogradXPU: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:19039 [autograd kernel]\nAutogradHPU: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:19039 [autograd kernel]\nAutogradVE: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:19039 [autograd kernel]\nAutogradLazy: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:19039 [autograd kernel]\nAutogradMTIA: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:19039 [autograd kernel]\nAutogradPrivateUse1: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:19039 [autograd kernel]\nAutogradPrivateUse2: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:19039 [autograd kernel]\nAutogradPrivateUse3: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:19039 [autograd kernel]\nAutogradMeta: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:19039 [autograd kernel]\nAutogradNestedTensor: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\VariableType_2.cpp:19039 [autograd kernel]\nTracer: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\generated\\TraceType_3.cpp:14672 [kernel]\nAutocastCPU: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\autocast_mode.cpp:378 [backend fallback]\nAutocastCUDA: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\autocast_mode.cpp:244 [backend fallback]\nFuncTorchBatched: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\functorch\\LegacyBatchingRegistrations.cpp:720 [backend fallback]\nBatchedNestedTensor: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\functorch\\LegacyBatchingRegistrations.cpp:746 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\functorch\\VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\functorch\\TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:162 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\functorch\\DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:166 [backend fallback]\nPythonDispatcher: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:158 [backend fallback]\n"
     ]
    }
   ],
   "source": [
    "print('Testing the model after quantization')\n",
    "test(net_quantized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.1+cu121'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 cls_token torch.Size([1, 1, 32])\n",
      "1 pos_embed torch.Size([1, 50, 32])\n",
      "2 patch_embed.proj.weight torch.Size([32, 1, 4, 4])\n",
      "3 patch_embed.proj.bias torch.Size([32])\n",
      "4 blocks.0.norm1.weight torch.Size([32])\n",
      "5 blocks.0.norm1.bias torch.Size([32])\n",
      "6 blocks.0.attn.qkv.weight torch.Size([96, 32])\n",
      "7 blocks.0.attn.qkv.bias torch.Size([96])\n",
      "8 blocks.0.attn.proj.weight torch.Size([32, 32])\n",
      "9 blocks.0.attn.proj.bias torch.Size([32])\n",
      "10 blocks.0.norm2.weight torch.Size([32])\n",
      "11 blocks.0.norm2.bias torch.Size([32])\n",
      "12 blocks.0.mlp.fc1.weight torch.Size([12, 32])\n",
      "13 blocks.0.mlp.fc1.bias torch.Size([12])\n",
      "14 blocks.0.mlp.fc2.weight torch.Size([32, 12])\n",
      "15 blocks.0.mlp.fc2.bias torch.Size([32])\n",
      "16 blocks.1.norm1.weight torch.Size([32])\n",
      "17 blocks.1.norm1.bias torch.Size([32])\n",
      "18 blocks.1.attn.qkv.weight torch.Size([96, 32])\n",
      "19 blocks.1.attn.qkv.bias torch.Size([96])\n",
      "20 blocks.1.attn.proj.weight torch.Size([32, 32])\n",
      "21 blocks.1.attn.proj.bias torch.Size([32])\n",
      "22 blocks.1.norm2.weight torch.Size([32])\n",
      "23 blocks.1.norm2.bias torch.Size([32])\n",
      "24 blocks.1.mlp.fc1.weight torch.Size([12, 32])\n",
      "25 blocks.1.mlp.fc1.bias torch.Size([12])\n",
      "26 blocks.1.mlp.fc2.weight torch.Size([32, 12])\n",
      "27 blocks.1.mlp.fc2.bias torch.Size([32])\n",
      "28 norm.weight torch.Size([32])\n",
      "29 norm.bias torch.Size([32])\n",
      "30 head.weight torch.Size([10, 32])\n",
      "31 head.bias torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for i, (name, param) in enumerate(MNIST_ViT.named_parameters()):\n",
    "    print(i, name,param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 cls_token torch.Size([1, 1, 32])\n",
      "1 pos_embed torch.Size([1, 50, 32])\n",
      "2 blocks.0.norm1.weight torch.Size([32])\n",
      "3 blocks.0.norm1.bias torch.Size([32])\n",
      "4 blocks.0.norm2.weight torch.Size([32])\n",
      "5 blocks.0.norm2.bias torch.Size([32])\n",
      "6 blocks.1.norm1.weight torch.Size([32])\n",
      "7 blocks.1.norm1.bias torch.Size([32])\n",
      "8 blocks.1.norm2.weight torch.Size([32])\n",
      "9 blocks.1.norm2.bias torch.Size([32])\n",
      "10 norm.weight torch.Size([32])\n",
      "11 norm.bias torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "for i, (name, param) in enumerate(net_quantized.named_parameters()):\n",
    "    print(i, name,param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
